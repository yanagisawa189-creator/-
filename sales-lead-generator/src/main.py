#!/usr/bin/env python3
"""
å–¶æ¥­ãƒªã‚¹ãƒˆè‡ªå‹•ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
"""

import asyncio
import argparse
import logging
import sys
from typing import Dict, List
from datetime import datetime

from config.config import config
from models import SearchQuery
from search_engine import SearchEngine, QueryBuilder
from scraper import WebScraper
from claude_extractor import ClaudeExtractor
from data_enhancer import DataEnhancer
from scorer import LeadScorer, ScoreAnalyzer
from exporters import DataExporter, CSVTemplateGenerator
from crm_integrations import CRMIntegrationManager
from history_manager import HistoryManager

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sales_lead_generator.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class SalesLeadGenerator:
    def __init__(self):
        self.search_engine = SearchEngine()
        self.claude_extractor = ClaudeExtractor()
        self.data_enhancer = DataEnhancer()
        self.scorer = LeadScorer()
        self.exporter = DataExporter()
        self.crm_manager = CRMIntegrationManager()
        self.history_manager = HistoryManager()

    async def generate_leads(
        self,
        industry: str,
        location: str,
        additional_keywords: List[str] = None,
        max_results: int = None,
        export_formats: List[str] = None,
        sync_to_crm: bool = False,
        wordpress_only: bool = False,
        exclude_history: bool = True
    ) -> Dict:
        """
        å–¶æ¥­ãƒªãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†
        """
        logger.info(f"Starting lead generation for industry: {industry}, location: {location}")

        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
            search_queries = self._build_search_queries(industry, location, additional_keywords)
            logger.info(f"Generated {len(search_queries)} search queries")

            # ã‚¹ãƒ†ãƒƒãƒ—2: æ¤œç´¢å®Ÿè¡Œ
            all_search_results = []
            for query in search_queries:
                search_results = await self.search_engine.search(query)
                all_search_results.extend(search_results)
                logger.info(f"Query '{query.to_search_string()}' returned {len(search_results)} results")

            if not all_search_results:
                logger.warning("No search results found")
                return {"error": "No search results found", "success": False}

            # é‡è¤‡ã‚’é™¤å»
            unique_results = self._deduplicate_search_results(all_search_results)
            logger.info(f"After deduplication: {len(unique_results)} unique results")

            # çµæœæ•°ã‚’åˆ¶é™
            if max_results and len(unique_results) > max_results:
                unique_results = unique_results[:max_results]
                logger.info(f"Limited to {max_results} results")

            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            logger.info("Starting web scraping...")
            async with WebScraper() as scraper:
                scraped_data = await scraper.scrape_urls(unique_results)

            logger.info(f"Successfully scraped {len(scraped_data)} pages")

            # WordPressãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæŒ‡å®šæ™‚ã®ã¿ï¼‰
            if wordpress_only:
                wordpress_data = [data for data in scraped_data if data.get('is_wordpress', False)]
                logger.info(f"WordPress filtering: {len(wordpress_data)} WordPress sites found out of {len(scraped_data)} total")
                scraped_data = wordpress_data

            if not scraped_data:
                error_msg = "No WordPress sites found" if wordpress_only else "No data could be scraped"
                logger.warning(error_msg)
                return {"error": error_msg, "success": False}

            # ã‚¹ãƒ†ãƒƒãƒ—4: Claude ã«ã‚ˆã‚‹æƒ…å ±æŠ½å‡º
            logger.info("Starting Claude extraction...")
            companies = await self.claude_extractor.extract_company_info_batch(scraped_data)
            logger.info(f"Extracted information for {len(companies)} companies")

            if not companies:
                logger.warning("No company information extracted")
                return {"error": "No company information extracted", "success": False}

            # ã‚¹ãƒ†ãƒƒãƒ—5: å±¥æ­´ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if exclude_history:
                logger.info("Checking against history...")
                original_count = len(companies)
                companies = self.history_manager.filter_new_companies(companies)
                filtered_count = original_count - len(companies)
                logger.info(f"Filtered out {filtered_count} duplicate companies from history")

                if not companies:
                    logger.warning("All companies were duplicates from history")
                    return {"error": "ã™ã¹ã¦ã®ä¼æ¥­ãŒå±¥æ­´ã«å­˜åœ¨ã—ã¾ã™ã€‚æ–°ã—ã„ä¼æ¥­ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "success": False}

            # ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
            logger.info("Enhancing company data...")
            enhanced_companies = await self.data_enhancer.enhance_companies(companies)
            logger.info(f"Enhanced {len(enhanced_companies)} companies")

            # ã‚¹ãƒ†ãƒƒãƒ—7: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            logger.info("Scoring leads...")
            search_query = search_queries[0]  # æœ€åˆã®ã‚¯ã‚¨ãƒªã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨
            scored_leads = self.scorer.score_leads(enhanced_companies, search_query)
            logger.info(f"Scored {len(scored_leads)} leads")

            # ã‚¹ãƒ†ãƒƒãƒ—8: å±¥æ­´ã«è¿½åŠ 
            search_query_str = f"{industry} {location}"
            if additional_keywords:
                search_query_str += " " + " ".join(additional_keywords)
            added_count = self.history_manager.add_companies(enhanced_companies, search_query_str)
            logger.info(f"Added {added_count} new companies to history")

            # çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
            stats = ScoreAnalyzer.analyze_score_distribution(scored_leads)
            logger.info(f"Statistics: {stats}")

            # ã‚¹ãƒ†ãƒƒãƒ—9: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            search_info = {
                "industry": industry,
                "location": location,
                "additional_keywords": additional_keywords or []
            }

            export_results = {}
            if not export_formats:
                export_formats = ['csv', 'excel']

            if 'csv' in export_formats or 'all' in export_formats:
                csv_path = await self.exporter.export_to_csv(scored_leads, search_info=search_info)
                if csv_path:
                    export_results['csv'] = csv_path

            if 'excel' in export_formats or 'all' in export_formats:
                excel_path = await self.exporter.export_to_excel(scored_leads, search_info=search_info)
                if excel_path:
                    export_results['excel'] = excel_path

            if 'sqlite' in export_formats or 'all' in export_formats:
                sqlite_path = await self.exporter.export_to_sqlite(scored_leads, search_info=search_info)
                if sqlite_path:
                    export_results['sqlite'] = sqlite_path

            logger.info(f"Exported to: {list(export_results.keys())}")

            # ã‚¹ãƒ†ãƒƒãƒ—10: CRMé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            crm_results = {}
            if sync_to_crm:
                logger.info("Syncing to CRM systems...")
                crm_results = await self.crm_manager.sync_to_all_crms(scored_leads)
                logger.info(f"CRM sync results: {crm_results}")

            # çµæœã®é›†ç´„
            result = {
                "success": True,
                "timestamp": datetime.now().isoformat(),
                "search_info": search_info,
                "statistics": stats,
                "leads_count": len(scored_leads),
                "top_leads": [lead.to_dict() for lead in ScoreAnalyzer.get_top_leads(scored_leads, 10)],
                "export_results": export_results,
                "crm_results": crm_results
            }

            logger.info("Lead generation completed successfully")
            return result

        except Exception as e:
            logger.error(f"Error in lead generation: {e}", exc_info=True)
            return {"error": str(e), "success": False}

    def _build_search_queries(self, industry: str, location: str, additional_keywords: List[str] = None) -> List[SearchQuery]:
        """
        æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        """
        queries = []

        # åŸºæœ¬ã‚¯ã‚¨ãƒª
        base_query = SearchQuery(
            industry=industry,
            location=location,
            additional_keywords=additional_keywords or []
        )
        queries.append(base_query)

        # æ¥­ç¨®ç‰¹åŒ–ã‚¯ã‚¨ãƒª
        industry_queries = QueryBuilder.build_industry_queries(industry, location)
        queries.extend(industry_queries[:3])  # æœ€å¤§3ã¤ã®è¿½åŠ ã‚¯ã‚¨ãƒª

        return queries

    def _deduplicate_search_results(self, search_results):
        """
        æ¤œç´¢çµæœã®é‡è¤‡ã‚’é™¤å»
        """
        seen_urls = set()
        unique_results = []

        for result in search_results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)

        return unique_results

async def main():
    """
    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    parser = argparse.ArgumentParser(description="å–¶æ¥­ãƒªã‚¹ãƒˆè‡ªå‹•ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")

    parser.add_argument("--industry", "-i", required=True, help="å¯¾è±¡æ¥­ç¨®")
    parser.add_argument("--location", "-l", required=True, help="å¯¾è±¡ã‚¨ãƒªã‚¢")
    parser.add_argument("--keywords", "-k", nargs="*", help="è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    parser.add_argument("--max-results", "-m", type=int, default=50, help="æœ€å¤§çµæœæ•°")
    parser.add_argument("--export", "-e", nargs="*", choices=["csv", "excel", "sqlite", "all"],
                      default=["csv", "excel"], help="ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼")
    parser.add_argument("--sync-crm", action="store_true", help="CRMã«åŒæœŸ")
    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°ãƒ­ã‚°")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # è¨­å®šãƒã‚§ãƒƒã‚¯
    required_configs = []
    if not config.claude.api_key:
        required_configs.append("ANTHROPIC_API_KEY")
    if not config.search.google_api_key and not config.search.serpapi_key:
        required_configs.append("GOOGLE_API_KEY or SERPAPI_KEY")

    if required_configs:
        logger.error(f"Required configuration missing: {', '.join(required_configs)}")
        print(f"ã‚¨ãƒ©ãƒ¼: å¿…è¦ãªè¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(required_configs)}")
        print("è©³ç´°ã¯.env.exampleãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # å®Ÿè¡Œ
    generator = SalesLeadGenerator()
    result = await generator.generate_leads(
        industry=args.industry,
        location=args.location,
        additional_keywords=args.keywords,
        max_results=args.max_results,
        export_formats=args.export,
        sync_to_crm=args.sync_crm
    )

    # çµæœå‡ºåŠ›
    if result["success"]:
        print(f"\nâœ… å–¶æ¥­ãƒªã‚¹ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"   ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ¼ãƒ‰æ•°: {result['leads_count']}")
        print(f"   é«˜å„ªå…ˆåº¦ãƒªãƒ¼ãƒ‰: {result['statistics'].get('high_priority_leads', 0)}")
        print(f"   ä¸­å„ªå…ˆåº¦ãƒªãƒ¼ãƒ‰: {result['statistics'].get('medium_priority_leads', 0)}")
        print(f"   ä½å„ªå…ˆåº¦ãƒªãƒ¼ãƒ‰: {result['statistics'].get('low_priority_leads', 0)}")

        if result.get('export_results'):
            print(f"\nğŸ“ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«:")
            for format_type, filepath in result['export_results'].items():
                print(f"   {format_type.upper()}: {filepath}")

        if result.get('crm_results'):
            print(f"\nğŸ”„ CRMåŒæœŸçµæœ:")
            for crm_name, crm_result in result['crm_results'].items():
                if crm_result.get('success'):
                    print(f"   {crm_name}: {crm_result.get('created', 0)} ä»¶ä½œæˆ")
                else:
                    print(f"   {crm_name}: ã‚¨ãƒ©ãƒ¼")

        print(f"\nğŸ¯ ä¸Šä½5ä»¶ã®ãƒªãƒ¼ãƒ‰:")
        for i, lead in enumerate(result['top_leads'][:5], 1):
            print(f"   {i}. {lead['company_name']} (ã‚¹ã‚³ã‚¢: {lead['total_score']:.1f})")
    else:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {result['error']}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())