#!/usr/bin/env python3
"""
ç™ºè¦‹ã—ãŸé˜²ç½è¨ˆç”»ãƒšãƒ¼ã‚¸ã‹ã‚‰PDF/HTMLã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import csv
import requests
import os
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time
from pathlib import Path
import hashlib

class DisasterPlanDownloader:
    def __init__(self, urls_csv="data/registry/disaster_plan_urls.csv"):
        self.urls_csv = urls_csv
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.download_results = []
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.pdf_dir = Path("data/raw_pdfs")
        self.html_dir = Path("data/raw_html")
        self.pdf_dir.mkdir(exist_ok=True)
        self.html_dir.mkdir(exist_ok=True)
        
    def load_disaster_urls(self):
        """é˜²ç½è¨ˆç”»URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        urls = []
        with open(self.urls_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['disaster_plan_url'] and row['disaster_plan_url'] != 'None':
                    urls.append(row)
        return urls
        
    def download_disaster_content(self, url_info):
        """é˜²ç½è¨ˆç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        municipality_name = url_info['municipality_name']
        municipality_id = url_info['municipality_id']
        disaster_url = url_info['disaster_plan_url']
        
        print(f"\nğŸ“¥ {municipality_name} ã®é˜²ç½è¨ˆç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print(f"    URL: {disaster_url}")
        
        try:
            response = self.session.get(disaster_url, timeout=15)
            response.raise_for_status()
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            content_type = response.headers.get('content-type', '').lower()
            
            if 'pdf' in content_type:
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                return self.save_pdf(response, municipality_id, municipality_name, disaster_url)
                
            elif 'html' in content_type or 'text' in content_type:
                # HTMLãƒšãƒ¼ã‚¸ã®å ´åˆã€PDF ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                return self.process_html_page(response, municipality_id, municipality_name, disaster_url)
                
            else:
                print(f"  â“ ä¸æ˜ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—: {content_type}")
                return {
                    'municipality_id': municipality_id,
                    'municipality_name': municipality_name,
                    'source_url': disaster_url,
                    'status': 'unknown_content_type',
                    'file_path': None,
                    'file_hash': None,
                    'content_type': content_type
                }
                
        except Exception as e:
            print(f"  âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'municipality_id': municipality_id,
                'municipality_name': municipality_name,
                'source_url': disaster_url,
                'status': 'download_error',
                'file_path': None,
                'file_hash': None,
                'error_message': str(e)
            }
            
    def save_pdf(self, response, municipality_id, municipality_name, source_url):
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        filename = f"{municipality_id}_disaster_plan.pdf"
        file_path = self.pdf_dir / filename
        
        with open(file_path, 'wb') as f:
            f.write(response.content)
            
        # ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        file_hash = hashlib.sha256(response.content).hexdigest()
        
        print(f"  âœ… PDFä¿å­˜å®Œäº†: {file_path}")
        
        return {
            'municipality_id': municipality_id,
            'municipality_name': municipality_name,
            'source_url': source_url,
            'status': 'pdf_downloaded',
            'file_path': str(file_path),
            'file_hash': file_hash,
            'content_type': 'application/pdf',
            'file_size': len(response.content)
        }
        
    def process_html_page(self, response, municipality_id, municipality_name, source_url):
        """HTMLãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™"""
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # PDF ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        pdf_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text(strip=True)
            
            if href.lower().endswith('.pdf') or 'pdf' in text.lower():
                full_url = urljoin(source_url, href)
                if any(keyword in text for keyword in ['é˜²ç½è¨ˆç”»', 'é¿é›£', 'ç½å®³']):
                    pdf_links.append({
                        'url': full_url,
                        'text': text
                    })
                    
        if pdf_links:
            print(f"  ğŸ“‹ {len(pdf_links)} ä»¶ã®PDFãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
            # æœ€åˆã®PDFãƒªãƒ³ã‚¯ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            pdf_link = pdf_links[0]
            try:
                pdf_response = self.session.get(pdf_link['url'], timeout=15)
                pdf_response.raise_for_status()
                return self.save_pdf(pdf_response, municipality_id, municipality_name, pdf_link['url'])
            except Exception as e:
                print(f"  âŒ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # PDFãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯HTMLã‚’ä¿å­˜
        filename = f"{municipality_id}_disaster_page.html"
        file_path = self.html_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
            
        # ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        file_hash = hashlib.sha256(response.content).hexdigest()
        
        print(f"  ğŸ“ HTMLä¿å­˜å®Œäº†: {file_path}")
        
        return {
            'municipality_id': municipality_id,
            'municipality_name': municipality_name,
            'source_url': source_url,
            'status': 'html_saved',
            'file_path': str(file_path),
            'file_hash': file_hash,
            'content_type': 'text/html',
            'file_size': len(response.content),
            'pdf_links_found': len(pdf_links)
        }
        
    def save_download_results(self):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’ä¿å­˜"""
        output_file = "data/registry/disaster_plan_downloads.csv"
        
        if self.download_results:
            fieldnames = list(self.download_results[0].keys())
            
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.download_results)
                
        print(f"\nğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’ä¿å­˜: {output_file}")
        
        # çµ±è¨ˆè¡¨ç¤º
        total = len(self.download_results)
        pdf_count = sum(1 for r in self.download_results if r['status'] == 'pdf_downloaded')
        html_count = sum(1 for r in self.download_results if r['status'] == 'html_saved')
        
        print(f"ç·ä»¶æ•°: {total}")
        print(f"PDFå–å¾—: {pdf_count} ä»¶")
        print(f"HTMLä¿å­˜: {html_count} ä»¶")
        
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ é˜²ç½è¨ˆç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹")
        
        disaster_urls = self.load_disaster_urls()
        print(f"å¯¾è±¡URLæ•°: {len(disaster_urls)}")
        
        for url_info in disaster_urls:
            result = self.download_disaster_content(url_info)
            self.download_results.append(result)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(2)
            
        self.save_download_results()
        print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")

if __name__ == "__main__":
    downloader = DisasterPlanDownloader()
    downloader.run()