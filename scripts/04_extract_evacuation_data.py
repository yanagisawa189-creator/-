#!/usr/bin/env python3
"""
é˜²ç½è¨ˆç”»PDF/HTMLã‹ã‚‰é¿é›£æ‰€æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import csv
import json
import re
import os
from pathlib import Path
import pdfplumber
from bs4 import BeautifulSoup

class EvacuationDataExtractor:
    def __init__(self):
        self.pdf_dir = Path("data/raw_pdfs")
        self.html_dir = Path("data/raw_html")
        self.extraction_results = []
        
        # é¿é›£æ‰€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.shelter_keywords = [
            'æŒ‡å®šé¿é›£æ‰€', 'é¿é›£æ‰€', 'ç·Šæ€¥é¿é›£å ´æ‰€', 'æ´¥æ³¢é¿é›£ãƒ“ãƒ«', 
            'ä¸€æ™‚é¿é›£å ´æ‰€', 'åºƒåŸŸé¿é›£å ´æ‰€', 'é¿é›£å ´æ‰€'
        ]
        
        # ç½å®³ç¨®åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.hazard_keywords = {
            'æ´¥æ³¢': ['æ´¥æ³¢'],
            'æ´ªæ°´': ['æ´ªæ°´', 'æ²³å·æ°¾æ¿«', 'å†…æ°´æ°¾æ¿«'],
            'åœŸç ‚ç½å®³': ['åœŸç ‚ç½å®³', 'åœŸçŸ³æµ', 'åœ°ã™ã¹ã‚Š', 'æ€¥å‚¾æ–œåœ°å´©å£Š'],
            'åœ°éœ‡': ['åœ°éœ‡', 'éœ‡ç½'],
            'é«˜æ½®': ['é«˜æ½®'],
            'ç«ç½': ['ç«ç½', 'å¤§ç«']
        }
        
    def extract_from_pdf(self, pdf_path, municipality_info):
        """PDFã‹ã‚‰é¿é›£æ‰€æƒ…å ±ã‚’æŠ½å‡º"""
        municipality_name = municipality_info['municipality_name']
        municipality_id = municipality_info['municipality_id']
        
        print(f"ğŸ“„ {municipality_name} ã®PDFã‚’è§£æä¸­: {pdf_path}")
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                full_text = ""
                tables = []
                
                for page in pdf.pages:
                    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
                    page_tables = page.extract_tables()
                    if page_tables:
                        tables.extend(page_tables)
                
                return self.analyze_extracted_content(
                    full_text, tables, municipality_id, municipality_name, 'pdf', str(pdf_path)
                )
                
        except Exception as e:
            print(f"  âŒ PDFè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'municipality_id': municipality_id,
                'municipality_name': municipality_name,
                'source_file': str(pdf_path),
                'extraction_status': 'pdf_error',
                'error_message': str(e)
            }
            
    def extract_from_html(self, html_path, municipality_info):
        """HTMLã‹ã‚‰é¿é›£æ‰€æƒ…å ±ã‚’æŠ½å‡º"""
        municipality_name = municipality_info['municipality_name']
        municipality_id = municipality_info['municipality_id']
        
        print(f"ğŸŒ {municipality_name} ã®HTMLã‚’è§£æä¸­: {html_path}")
        
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            soup = BeautifulSoup(content, 'html.parser')
            
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            full_text = soup.get_text()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
            tables = []
            for table in soup.find_all('table'):
                table_data = []
                for row in table.find_all('tr'):
                    row_data = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]
                    if row_data:
                        table_data.append(row_data)
                if table_data:
                    tables.append(table_data)
                    
            return self.analyze_extracted_content(
                full_text, tables, municipality_id, municipality_name, 'html', str(html_path)
            )
            
        except Exception as e:
            print(f"  âŒ HTMLè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'municipality_id': municipality_id,
                'municipality_name': municipality_name,
                'source_file': str(html_path),
                'extraction_status': 'html_error',
                'error_message': str(e)
            }
            
    def analyze_extracted_content(self, text, tables, municipality_id, municipality_name, source_type, source_file):
        """æŠ½å‡ºã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰é¿é›£æ‰€æƒ…å ±ã‚’åˆ†æ"""
        result = {
            'municipality_id': municipality_id,
            'municipality_name': municipality_name,
            'source_file': source_file,
            'source_type': source_type,
            'extraction_status': 'analyzed'
        }
        
        # é¿é›£æ‰€æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        shelter_counts = self.count_shelters_in_text(text, tables)
        result.update(shelter_counts)
        
        # å¯¾å¿œç½å®³ç¨®åˆ¥ã‚’æŠ½å‡º
        hazard_types = self.extract_hazard_types(text)
        result['supported_hazards'] = ','.join(hazard_types)
        
        # å…·ä½“çš„ãªé¿é›£æ‰€ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        shelter_list = self.extract_shelter_list(text, tables)
        result['shelter_details'] = json.dumps(shelter_list, ensure_ascii=False) if shelter_list else None
        
        print(f"  âœ… è§£æå®Œäº†: æŒ‡å®šé¿é›£æ‰€{shelter_counts.get('designated_shelters', 0)}ç®‡æ‰€")
        
        return result
        
    def count_shelters_in_text(self, text, tables):
        """ãƒ†ã‚­ã‚¹ãƒˆã¨è¡¨ã‹ã‚‰é¿é›£æ‰€æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = {
            'designated_shelters': 0,
            'emergency_evacuation_sites': 0,
            'tsunami_evacuation_buildings': 0,
            'total_facilities': 0
        }
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
        patterns = [
            (r'æŒ‡å®šé¿é›£æ‰€[ï¼š:]?\s*(\d+)[ç®‡ã‹æ‰€]', 'designated_shelters'),
            (r'ç·Šæ€¥é¿é›£å ´æ‰€[ï¼š:]?\s*(\d+)[ç®‡ã‹æ‰€]', 'emergency_evacuation_sites'),
            (r'æ´¥æ³¢é¿é›£ãƒ“ãƒ«[ï¼š:]?\s*(\d+)[æ£Ÿç®‡ã‹æ‰€]', 'tsunami_evacuation_buildings'),
        ]
        
        for pattern, key in patterns:
            matches = re.findall(pattern, text)
            if matches:
                counts[key] = max(int(match) for match in matches)
                
        # è¡¨ã‹ã‚‰é¿é›£æ‰€ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        table_counts = self.count_shelters_in_tables(tables)
        for key, value in table_counts.items():
            if value > counts[key]:
                counts[key] = value
                
        # ç·æ•°ã‚’è¨ˆç®—
        counts['total_facilities'] = sum([
            counts['designated_shelters'],
            counts['emergency_evacuation_sites'],
            counts['tsunami_evacuation_buildings']
        ])
        
        return counts
        
    def count_shelters_in_tables(self, tables):
        """è¡¨ã‹ã‚‰é¿é›£æ‰€ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = {
            'designated_shelters': 0,
            'emergency_evacuation_sites': 0,
            'tsunami_evacuation_buildings': 0
        }
        
        for table in tables:
            if not table:
                continue
                
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‹ã‚‰é¿é›£æ‰€é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆ¤å®š
            header = ' '.join(table[0]) if table else ''
            
            if any(keyword in header for keyword in self.shelter_keywords):
                # é¿é›£æ‰€ãƒªã‚¹ãƒˆã®è¡¨ã¨ã—ã¦æ‰±ã†
                shelter_rows = [row for row in table[1:] if row and any(cell.strip() for cell in row)]
                
                if 'æŒ‡å®šé¿é›£æ‰€' in header:
                    counts['designated_shelters'] = max(counts['designated_shelters'], len(shelter_rows))
                elif 'ç·Šæ€¥é¿é›£' in header:
                    counts['emergency_evacuation_sites'] = max(counts['emergency_evacuation_sites'], len(shelter_rows))
                elif 'æ´¥æ³¢' in header:
                    counts['tsunami_evacuation_buildings'] = max(counts['tsunami_evacuation_buildings'], len(shelter_rows))
                    
        return counts
        
    def extract_hazard_types(self, text):
        """å¯¾å¿œç½å®³ç¨®åˆ¥ã‚’æŠ½å‡º"""
        found_hazards = []
        
        for hazard_type, keywords in self.hazard_keywords.items():
            if any(keyword in text for keyword in keywords):
                found_hazards.append(hazard_type)
                
        return found_hazards
        
    def extract_shelter_list(self, text, tables):
        """å…·ä½“çš„ãªé¿é›£æ‰€ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        shelters = []
        
        # è¡¨ã‹ã‚‰é¿é›£æ‰€æƒ…å ±ã‚’æŠ½å‡º
        for table in tables:
            if not table or len(table) < 2:
                continue
                
            header = table[0]
            
            # é¿é›£æ‰€ãƒªã‚¹ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆ¤å®š
            if any(keyword in ' '.join(header) for keyword in self.shelter_keywords):
                for row in table[1:]:
                    if row and len(row) >= 1 and row[0].strip():
                        shelter_info = {
                            'name': row[0].strip(),
                            'address': row[1].strip() if len(row) > 1 else '',
                            'capacity': row[2].strip() if len(row) > 2 else '',
                            'notes': row[3].strip() if len(row) > 3 else ''
                        }
                        shelters.append(shelter_info)
                        
        return shelters[:10]  # æœ€å¤§10ä»¶ã¾ã§
        
    def load_download_results(self):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã‚’èª­ã¿è¾¼ã¿"""
        downloads_csv = "data/registry/disaster_plan_downloads.csv"
        
        if not os.path.exists(downloads_csv):
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {downloads_csv}")
            return []
            
        downloads = []
        with open(downloads_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            downloads = list(reader)
            
        return downloads
        
    def save_extraction_results(self):
        """æŠ½å‡ºçµæœã‚’ä¿å­˜"""
        output_file = "data/normalized/okinawa_evacuation_data.csv"
        
        if self.extraction_results:
            fieldnames = list(self.extraction_results[0].keys())
            
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.extraction_results)
                
        print(f"\nğŸ’¾ æŠ½å‡ºçµæœã‚’ä¿å­˜: {output_file}")
        
        # çµ±è¨ˆè¡¨ç¤º
        total = len(self.extraction_results)
        success_count = sum(1 for r in self.extraction_results if r.get('extraction_status') == 'analyzed')
        
        print(f"ç·ä»¶æ•°: {total}")
        print(f"è§£ææˆåŠŸ: {success_count} ä»¶")
        
        if success_count > 0:
            total_shelters = sum(r.get('designated_shelters', 0) for r in self.extraction_results)
            print(f"ç™ºè¦‹ã—ãŸæŒ‡å®šé¿é›£æ‰€ç·æ•°: {total_shelters} ç®‡æ‰€")
            
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹")
        
        downloads = self.load_download_results()
        print(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(downloads)}")
        
        for download in downloads:
            municipality_info = {
                'municipality_id': download['municipality_id'],
                'municipality_name': download['municipality_name']
            }
            
            file_path = download.get('file_path')
            if not file_path or not os.path.exists(file_path):
                print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                continue
                
            if download['status'] == 'pdf_downloaded':
                result = self.extract_from_pdf(file_path, municipality_info)
            elif download['status'] == 'html_saved':
                result = self.extract_from_html(file_path, municipality_info)
            else:
                continue
                
            self.extraction_results.append(result)
            
        self.save_extraction_results()
        print("âœ… æŠ½å‡ºå®Œäº†")

if __name__ == "__main__":
    extractor = EvacuationDataExtractor()
    extractor.run()