#!/usr/bin/env python3
"""
æ²–ç¸„æœ¬å³¶ã®å¸‚ç”ºæ‘ã®åœ°åŸŸé˜²ç½è¨ˆç”»ã‚’æ¢ç´¢ãƒ»åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import csv
import json
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
import os
from pathlib import Path

class DisasterPlanDiscoverer:
    def __init__(self, municipalities_csv="data/registry/okinawa_municipalities.csv"):
        self.municipalities_csv = municipalities_csv
        self.municipalities = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # é˜²ç½è¨ˆç”»é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.keywords = [
            "åœ°åŸŸé˜²ç½è¨ˆç”»", "é˜²ç½è¨ˆç”»", "ç½å®³å¯¾ç­–", "é¿é›£æ‰€", "é¿é›£å ´æ‰€",
            "ç·Šæ€¥é¿é›£", "æŒ‡å®šé¿é›£æ‰€", "æ´¥æ³¢é¿é›£", "æ´ªæ°´", "åœŸç ‚ç½å®³"
        ]
        
        self.results = []
        
    def load_municipalities(self):
        """å¸‚ç”ºæ‘ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        with open(self.municipalities_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            self.municipalities = list(reader)
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(self.municipalities)} å¸‚ç”ºæ‘")
        
    def search_disaster_plan_page(self, municipality):
        """å„å¸‚ç”ºæ‘ã®é˜²ç½è¨ˆç”»ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢"""
        base_url = municipality['official_site_url']
        municipality_name = municipality['municipality_name']
        
        print(f"\nğŸ” {municipality_name} ã®é˜²ç½è¨ˆç”»ã‚’æ¢ç´¢ä¸­...")
        
        try:
            # ã¾ãšã¯ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = self.session.get(base_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # é˜²ç½é–¢é€£ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            disaster_links = self.find_disaster_links(soup, base_url)
            
            if disaster_links:
                print(f"  âœ… {len(disaster_links)} ä»¶ã®é˜²ç½é–¢é€£ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹")
                for link in disaster_links:
                    self.results.append({
                        'municipality_id': municipality['municipality_id'],
                        'municipality_name': municipality_name,
                        'official_site_url': base_url,
                        'disaster_plan_url': link['url'],
                        'link_text': link['text'],
                        'discovery_method': 'homepage_link'
                    })
            else:
                # ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã‚’è©¦è¡Œ
                search_results = self.try_site_search(base_url, municipality_name)
                if search_results:
                    self.results.extend(search_results)
                else:
                    print(f"  âŒ é˜²ç½è¨ˆç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    self.results.append({
                        'municipality_id': municipality['municipality_id'],
                        'municipality_name': municipality_name,
                        'official_site_url': base_url,
                        'disaster_plan_url': None,
                        'link_text': None,
                        'discovery_method': 'not_found'
                    })
                    
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.results.append({
                'municipality_id': municipality['municipality_id'],
                'municipality_name': municipality_name,
                'official_site_url': base_url,
                'disaster_plan_url': None,
                'link_text': f"Error: {str(e)}",
                'discovery_method': 'error'
            })
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        time.sleep(2)
        
    def find_disaster_links(self, soup, base_url):
        """é˜²ç½é–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        disaster_links = []
        
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text(strip=True)
            
            # é˜²ç½é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            for keyword in self.keywords:
                if keyword in text or keyword in href:
                    full_url = urljoin(base_url, href)
                    disaster_links.append({
                        'url': full_url,
                        'text': text,
                        'keyword_matched': keyword
                    })
                    break
                    
        return disaster_links
        
    def try_site_search(self, base_url, municipality_name):
        """ä¸€èˆ¬çš„ãªé˜²ç½è¨ˆç”»URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ"""
        common_paths = [
            '/bosai/', '/disaster/', '/bousai/', '/é˜²ç½/',
            '/kikikanri/', '/anzen/', '/saigai/',
            '/kurashi/bosai/', '/kurashi/anzen/'
        ]
        
        results = []
        for path in common_paths:
            try:
                test_url = base_url.rstrip('/') + path
                response = self.session.head(test_url, timeout=5)
                if response.status_code == 200:
                    print(f"  âœ… æ¨æ¸¬URLç™ºè¦‹: {test_url}")
                    results.append({
                        'municipality_id': None,
                        'municipality_name': municipality_name,
                        'official_site_url': base_url,
                        'disaster_plan_url': test_url,
                        'link_text': f"æ¨æ¸¬ãƒ‘ã‚¹: {path}",
                        'discovery_method': 'path_guessing'
                    })
            except:
                continue
                
        return results
        
    def save_results(self):
        """çµæœã‚’ä¿å­˜"""
        output_file = "data/registry/disaster_plan_urls.csv"
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['municipality_id', 'municipality_name', 'official_site_url', 
                         'disaster_plan_url', 'link_text', 'discovery_method']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.results)
            
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {output_file}")
        print(f"ç·ä»¶æ•°: {len(self.results)} ä»¶")
        
        # çµ±è¨ˆã‚’è¡¨ç¤º
        found_count = sum(1 for r in self.results if r['disaster_plan_url'] is not None)
        print(f"ç™ºè¦‹æˆåŠŸ: {found_count} ä»¶ ({found_count/len(self.results)*100:.1f}%)")
        
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ æ²–ç¸„æœ¬å³¶å¸‚ç”ºæ‘ é˜²ç½è¨ˆç”»æ¢ç´¢ã‚’é–‹å§‹")
        
        self.load_municipalities()
        
        for municipality in self.municipalities:
            self.search_disaster_plan_page(municipality)
            
        self.save_results()
        print("âœ… æ¢ç´¢å®Œäº†")

if __name__ == "__main__":
    discoverer = DisasterPlanDiscoverer()
    discoverer.run()